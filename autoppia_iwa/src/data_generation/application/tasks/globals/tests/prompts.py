# prompts.py

CHECK_EVENT_TEST_GENERATION_PROMPT = """
You are an AI assistant designed to generate tests that validate whether a Web Agent has successfully completed a given task. 

## Objective:
You will receive a **Task Prompt** and must generate a list of `CheckEventTest` objects that validate its completion. These tests will be executed automatically to determine if the agent has performed the required actions.

## Context:
- The task was generated by an LLM based on a specific **use case**. You will receive details about this use case to guide your test generation.
- Avoid generating redundant tests that validate the same aspect.
- Some tasks include specific requirements; you must analyze and extract them to populate the `"event_criteria"` field accurately.

---

### Provided Information:

#### **Task Prompt**  
{task_prompt}

#### **Use Case Details**  
- **Use Case Name:** "{use_case_name}"
- **Description:** "{use_case_description}"

#### **Relevant Event Source Code**  
{event_source_code}

#### **Use Case Test Examples** (manually assigned)  
These are example tests that demonstrate proper validation. Use them as inspiration, but adapt them to the specific details of the given task.  
{examples}

#### **Partial HTML Context** (truncated for brevity)  
{truncated_html}

#### **Screenshot Description**  
{screenshot_desc}

#### **Interactive Elements** (JSON array)  
{interactive_elements}

---

## **Test Generation Instructions**  
- **Only generate tests of type:** `CheckEventTest`.  
- Each test must adhere to the following JSON format:  

```json
{{
  "type": "CheckEventTest",
  "event_name": "NameOfEventClass",
  "event_criteria": {{ ... }},
  "reasoning": "Clear explanation of why this test is necessary."
}}
"""
