# Estructura del Sistema de EvaluaciÃ³n

## ğŸ“ OrganizaciÃ³n de Carpetas

```
evaluation/
â”‚
â”œâ”€â”€ ğŸ“„ __init__.py                    # Exporta todos los evaluadores y clases
â”œâ”€â”€ ğŸ“„ classes.py                     # Clases comunes (EvaluationResult, EvaluatorConfig, etc.)
â”œâ”€â”€ ğŸ“„ interfaces.py                  # Interfaz IEvaluator
â”œâ”€â”€ ğŸ“„ README.md                      # DocumentaciÃ³n general
â”œâ”€â”€ ğŸ“„ STRUCTURE.md                   # Este archivo
â”‚
â”œâ”€â”€ ğŸ“‚ concurrent_evaluator/          # âš¡ Evaluador para soluciones completas
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ evaluator.py              # ConcurrentEvaluator
â”‚   â””â”€â”€ ğŸ“„ README.md                 # DocumentaciÃ³n especÃ­fica
â”‚
â”œâ”€â”€ ğŸ“‚ iterative_evaluator/           # ğŸ”„ Evaluador iterativo (agentes)
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ evaluator.py              # IterativeEvaluator
â”‚   â””â”€â”€ ğŸ“„ README.md                 # DocumentaciÃ³n especÃ­fica
â”‚
â”œâ”€â”€ ğŸ“‚ stateful_evaluator/            # ğŸ® Evaluador para RL/PPO
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ evaluator.py              # AsyncStatefulEvaluator, StatefulEvaluator
â”‚   â””â”€â”€ ğŸ“„ README.md                 # DocumentaciÃ³n especÃ­fica
â”‚
â””â”€â”€ ğŸ“‚ shared/                        # ğŸ”§ Utilidades compartidas
    â”œâ”€â”€ ğŸ“„ __init__.py
    â”œâ”€â”€ ğŸ“„ feedback_generator.py      # GeneraciÃ³n de feedback
    â”œâ”€â”€ ğŸ“„ test_runner.py            # EjecuciÃ³n de tests
    â””â”€â”€ ğŸ“„ utils.py                  # Funciones utilitarias
```

## ğŸ¯ Evaluadores

### 1. ConcurrentEvaluator âš¡

**UbicaciÃ³n:** `concurrent_evaluator/evaluator.py`

**PropÃ³sito:** Evaluar soluciones completas con todas las acciones ya generadas.

**Uso tÃ­pico:**
- Evaluar respuestas de mineros en Bittensor
- Benchmarks de agentes
- Testing de tasks

**Import:**
```python
from autoppia_iwa.src.evaluation import ConcurrentEvaluator
```

**CaracterÃ­sticas clave:**
- âœ… Recibe TaskSolution completa
- âœ… Agrupa soluciones idÃ©nticas
- âœ… EvaluaciÃ³n paralela
- âœ… Genera GIFs

---

### 2. IterativeEvaluator ğŸ”„

**UbicaciÃ³n:** `iterative_evaluator/evaluator.py`

**PropÃ³sito:** Evaluar agentes de forma iterativa, acciÃ³n por acciÃ³n.

**Uso tÃ­pico:**
- Agentes adaptativos
- Agentes que necesitan ver resultado de cada acciÃ³n
- EvaluaciÃ³n de agentes de RL

**Import:**
```python
from autoppia_iwa.src.evaluation import IterativeEvaluator
```

**CaracterÃ­sticas clave:**
- âœ… Llama al agente iterativamente
- âœ… Enriquece task con estado del browser
- âœ… Una acciÃ³n a la vez
- âœ… Agente puede adaptar estrategia

---

### 3. StatefulEvaluator ğŸ®

**UbicaciÃ³n:** `stateful_evaluator/evaluator.py`

**PropÃ³sito:** Para Reinforcement Learning y entrenamiento de agentes.

**Uso tÃ­pico:**
- Entrenamiento de RL (PPO, DQN, etc.)
- Testing de acciones individuales
- ExploraciÃ³n interactiva

**Import:**
```python
from autoppia_iwa.src.evaluation import AsyncStatefulEvaluator, StatefulEvaluator
```

**CaracterÃ­sticas clave:**
- âœ… Interfaz WebCUA
- âœ… Mantiene estado entre steps
- âœ… Score parcial despuÃ©s de cada acciÃ³n
- âœ… Versiones async y sync

---

## ğŸ”§ Utilidades Compartidas

**UbicaciÃ³n:** `shared/`

Todas las utilidades comunes usadas por los evaluadores:

### feedback_generator.py
- `FeedbackGenerator`: Genera feedback detallado de evaluaciones

### test_runner.py
- `TestRunner`: Ejecuta tests sobre tasks

### utils.py
- `generate_feedback()`: Genera feedback de evaluaciÃ³n
- `run_global_tests()`: Ejecuta todos los tests de una task
- `run_partial_tests()`: Ejecuta tests parciales (para RL)
- `make_gif_from_screenshots()`: Genera GIF de screenshots
- `hash_actions()`: Hash de acciones para agrupaciÃ³n
- `extract_seed_from_url()`: Extrae seed de URL
- Y mÃ¡s...

**Import:**
```python
from autoppia_iwa.src.evaluation.shared import (
    FeedbackGenerator,
    TestRunner,
    generate_feedback,
    run_global_tests,
)
```

---

## ğŸ“¦ Clases Comunes

**UbicaciÃ³n:** `classes.py`

Clases usadas por todos los evaluadores:

```python
from autoppia_iwa.src.evaluation import (
    EvaluationResult,      # Resultado de evaluaciÃ³n
    EvaluationStats,       # EstadÃ­sticas detalladas
    EvaluatorConfig,       # ConfiguraciÃ³n de evaluadores
    Feedback,              # Feedback de evaluaciÃ³n
    TestResult,            # Resultado de un test
)
```

---

## ğŸ”Œ Interfaz

**UbicaciÃ³n:** `interfaces.py`

```python
from autoppia_iwa.src.evaluation import IEvaluator
```

Todos los evaluadores implementan `IEvaluator`:

```python
class IEvaluator(ABC):
    @abstractmethod
    async def evaluate_single_task_solution(self, task: Task, task_solution: TaskSolution) -> EvaluationResult:
        ...
    
    @abstractmethod
    async def evaluate_task_solutions(self, task: Task, task_solutions: list[TaskSolution]) -> list[EvaluationResult]:
        ...
```

**Nota:** `IterativeEvaluator` no implementa estos mÃ©todos (usa `evaluate_with_agent`).

---

## ğŸš€ Imports Simplificados

Todos los evaluadores y utilidades estÃ¡n disponibles desde el import principal:

```python
# Evaluadores
from autoppia_iwa.src.evaluation import (
    ConcurrentEvaluator,
    IterativeEvaluator,
    AsyncStatefulEvaluator,
    StatefulEvaluator,
)

# Clases
from autoppia_iwa.src.evaluation import (
    EvaluationResult,
    EvaluationStats,
    EvaluatorConfig,
    Feedback,
    TestResult,
    IEvaluator,
)

# Utilidades
from autoppia_iwa.src.evaluation.shared import (
    FeedbackGenerator,
    TestRunner,
    generate_feedback,
    run_global_tests,
)
```

---

## ğŸ“š DocumentaciÃ³n

Cada evaluador tiene su propia documentaciÃ³n detallada:

- **General:** `evaluation/README.md`
- **ConcurrentEvaluator:** `concurrent_evaluator/README.md`
- **IterativeEvaluator:** `iterative_evaluator/README.md`
- **StatefulEvaluator:** `stateful_evaluator/README.md`

---

## ğŸ”„ ComparaciÃ³n RÃ¡pida

| CaracterÃ­stica | Concurrent | Iterative | Stateful |
|---------------|-----------|-----------|----------|
| **Input** | TaskSolution | IWebAgent | Task + actions |
| **Llamadas al agente** | 1 | N | Manual |
| **Estado persistente** | âŒ | âŒ | âœ… |
| **Score parcial** | âŒ | âŒ | âœ… |
| **AgrupaciÃ³n** | âœ… | âŒ | âŒ |
| **Paralelo** | âœ… | âŒ | âŒ |
| **Adaptativo** | âŒ | âœ… | âœ… |
| **RL/PPO** | âŒ | âŒ | âœ… |
| **Uso tÃ­pico** | ProducciÃ³n | Agentes adaptativos | Entrenamiento RL |

---

## âœ… Ventajas de esta Estructura

1. **EncapsulaciÃ³n:** Cada evaluador estÃ¡ en su propia carpeta con su documentaciÃ³n
2. **Claridad:** Es fÃ¡cil entender quÃ© hace cada evaluador
3. **Mantenibilidad:** Cambios en un evaluador no afectan a otros
4. **ReutilizaciÃ³n:** Utilidades compartidas en `shared/`
5. **DocumentaciÃ³n:** Cada carpeta tiene su README especÃ­fico
6. **Imports limpios:** Todo disponible desde `autoppia_iwa.src.evaluation`

---

## ğŸ”¨ Desarrollo

### AÃ±adir un nuevo evaluador

1. Crear carpeta: `nuevo_evaluador/`
2. Crear `evaluator.py` implementando `IEvaluator` (o interfaz custom)
3. Crear `__init__.py` exportando tu evaluador
4. Crear `README.md` con documentaciÃ³n
5. AÃ±adir export en `/evaluation/__init__.py`

### AÃ±adir utilidades compartidas

1. AÃ±adir funciÃ³n en `shared/utils.py`
2. O crear nuevo mÃ³dulo en `shared/`
3. Exportar en `shared/__init__.py`

---

## ğŸ“ Notas

- Todos los evaluadores comparten las mismas clases (`EvaluationResult`, `EvaluatorConfig`, etc.)
- Las utilidades en `shared/` estÃ¡n disponibles para todos
- Cada evaluador puede tener su propia configuraciÃ³n adicional
- La interfaz `IEvaluator` es opcional (IterativeEvaluator no la implementa)
