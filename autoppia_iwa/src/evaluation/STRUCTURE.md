# Estructura del Sistema de Evaluaci√≥n

## üìÅ Organizaci√≥n de Carpetas

```
evaluation/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ __init__.py                    # Exporta todos los evaluadores y clases
‚îú‚îÄ‚îÄ üìÑ classes.py                     # Clases comunes (EvaluationResult, EvaluatorConfig, etc.)
‚îú‚îÄ‚îÄ üìÑ interfaces.py                  # Interfaz IEvaluator
‚îú‚îÄ‚îÄ üìÑ README.md                      # Documentaci√≥n general
‚îú‚îÄ‚îÄ üìÑ STRUCTURE.md                   # Este archivo
‚îÇ
‚îú‚îÄ‚îÄ üìÇ concurrent_evaluator/          # ‚ö° Evaluador para soluciones completas
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ evaluator.py              # ConcurrentEvaluator
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ README.md                 # Documentaci√≥n espec√≠fica
‚îÇ
‚îú‚îÄ‚îÄ üìÇ stateful_evaluator/            # üéÆ Evaluador para modo iterativo/stateful
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ evaluator.py              # AsyncStatefulEvaluator, StatefulEvaluator
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ README.md                 # Documentaci√≥n espec√≠fica
‚îÇ
‚îî‚îÄ‚îÄ üìÇ shared/                        # üîß Utilidades compartidas
    ‚îú‚îÄ‚îÄ üìÑ __init__.py
    ‚îú‚îÄ‚îÄ üìÑ feedback_generator.py      # Generaci√≥n de feedback
    ‚îú‚îÄ‚îÄ üìÑ test_runner.py            # Ejecuci√≥n de tests
    ‚îî‚îÄ‚îÄ üìÑ utils.py                  # Funciones utilitarias
```

## üéØ Evaluadores

### 1. ConcurrentEvaluator ‚ö°

**Ubicaci√≥n:** `concurrent_evaluator/evaluator.py`

**Prop√≥sito:** Evaluar soluciones completas con todas las acciones ya generadas.

**Uso t√≠pico:**
- Evaluar respuestas de mineros en Bittensor
- Benchmarks de agentes
- Testing de tasks

**Import:**
```python
from autoppia_iwa.src.evaluation import ConcurrentEvaluator
```

**Caracter√≠sticas clave:**
- ‚úÖ Recibe TaskSolution completa
- ‚úÖ Agrupa soluciones id√©nticas
- ‚úÖ Evaluaci√≥n paralela
- ‚úÖ Genera GIFs

---

### 2. StatefulEvaluator üéÆ

**Ubicaci√≥n:** `stateful_evaluator/evaluator.py`

**Prop√≥sito:** Para evaluaci√≥n iterativa paso a paso (usado en subnet y modo stateful).

**Uso t√≠pico:**
- Evaluar miners remotos en la subnet (HTTP)
- Modo stateful del benchmark (agentes iterativos)
- Entrenamiento de RL (PPO, DQN, etc.)
- Testing de acciones individuales

**Import:**
```python
from autoppia_iwa.src.evaluation import AsyncStatefulEvaluator, StatefulEvaluator
```

**Caracter√≠sticas clave:**
- ‚úÖ Interfaz WebCUA
- ‚úÖ Mantiene estado entre steps
- ‚úÖ Score parcial despu√©s de cada acci√≥n
- ‚úÖ Versiones async y sync
- ‚úÖ Ejecuta m√∫ltiples acciones en batch (mejora de eficiencia)
- ‚úÖ Usado en la subnet para miners remotos

---

## üîß Utilidades Compartidas

**Ubicaci√≥n:** `shared/`

Todas las utilidades comunes usadas por los evaluadores:

### feedback_generator.py
- `FeedbackGenerator`: Genera feedback detallado de evaluaciones

### test_runner.py
- `TestRunner`: Ejecuta tests sobre tasks

### utils.py
- `generate_feedback()`: Genera feedback de evaluaci√≥n
- `run_global_tests()`: Ejecuta todos los tests de una task
- `run_partial_tests()`: Ejecuta tests parciales (para RL)
- `make_gif_from_screenshots()`: Genera GIF de screenshots
- `hash_actions()`: Hash de acciones para agrupaci√≥n
- `extract_seed_from_url()`: Extrae seed de URL
- Y m√°s...

**Import:**
```python
from autoppia_iwa.src.evaluation.shared import (
    FeedbackGenerator,
    TestRunner,
    generate_feedback,
    run_global_tests,
)
```

---

## üì¶ Clases Comunes

**Ubicaci√≥n:** `classes.py`

Clases usadas por todos los evaluadores:

```python
from autoppia_iwa.src.evaluation import (
    EvaluationResult,      # Resultado de evaluaci√≥n
    EvaluationStats,       # Estad√≠sticas detalladas
    EvaluatorConfig,       # Configuraci√≥n de evaluadores
    Feedback,              # Feedback de evaluaci√≥n
    TestResult,            # Resultado de un test
)
```

---

## üîå Interfaz

**Ubicaci√≥n:** `interfaces.py`

```python
from autoppia_iwa.src.evaluation import IEvaluator
```

Todos los evaluadores implementan `IEvaluator`:

```python
class IEvaluator(ABC):
    @abstractmethod
    async def evaluate_single_task_solution(self, task: Task, task_solution: TaskSolution) -> EvaluationResult:
        ...
    
    @abstractmethod
    async def evaluate_task_solutions(self, task: Task, task_solutions: list[TaskSolution]) -> list[EvaluationResult]:
        ...
```

**Nota:** `StatefulEvaluator` no implementa estos m√©todos (usa interfaz WebCUA con `reset()` y `step()`).

---

## üöÄ Imports Simplificados

Todos los evaluadores y utilidades est√°n disponibles desde el import principal:

```python
# Evaluadores
from autoppia_iwa.src.evaluation import (
    ConcurrentEvaluator,
    AsyncStatefulEvaluator,
    StatefulEvaluator,
)

# Clases
from autoppia_iwa.src.evaluation import (
    EvaluationResult,
    EvaluationStats,
    EvaluatorConfig,
    Feedback,
    TestResult,
    IEvaluator,
)

# Utilidades
from autoppia_iwa.src.evaluation.shared import (
    FeedbackGenerator,
    TestRunner,
    generate_feedback,
    run_global_tests,
)
```

---

## üìö Documentaci√≥n

Cada evaluador tiene su propia documentaci√≥n detallada:

- **General:** `evaluation/README.md`
- **ConcurrentEvaluator:** `concurrent_evaluator/README.md`
- **StatefulEvaluator:** `stateful_evaluator/README.md`

---

## üîÑ Comparaci√≥n R√°pida

| Caracter√≠stica | Concurrent | Stateful |
|---------------|-----------|----------|
| **Input** | TaskSolution | Task + actions |
| **Llamadas al agente** | 1 | N (iterativo) |
| **Estado persistente** | ‚ùå | ‚úÖ |
| **Score parcial** | ‚ùå | ‚úÖ |
| **Agrupaci√≥n** | ‚úÖ | ‚ùå |
| **Paralelo** | ‚úÖ | ‚ùå |
| **Adaptativo** | ‚ùå | ‚úÖ |
| **Batch actions** | N/A | ‚úÖ |
| **RL/PPO** | ‚ùå | ‚úÖ |
| **Uso t√≠pico** | Soluciones completas | Iterativo/Subnet/RL |

---

## ‚úÖ Ventajas de esta Estructura

1. **Encapsulaci√≥n:** Cada evaluador est√° en su propia carpeta con su documentaci√≥n
2. **Claridad:** Es f√°cil entender qu√© hace cada evaluador
3. **Mantenibilidad:** Cambios en un evaluador no afectan a otros
4. **Reutilizaci√≥n:** Utilidades compartidas en `shared/`
5. **Documentaci√≥n:** Cada carpeta tiene su README espec√≠fico
6. **Imports limpios:** Todo disponible desde `autoppia_iwa.src.evaluation`

---

## üî® Desarrollo

### A√±adir un nuevo evaluador

1. Crear carpeta: `nuevo_evaluador/`
2. Crear `evaluator.py` implementando `IEvaluator` (o interfaz custom)
3. Crear `__init__.py` exportando tu evaluador
4. Crear `README.md` con documentaci√≥n
5. A√±adir export en `/evaluation/__init__.py`

### A√±adir utilidades compartidas

1. A√±adir funci√≥n en `shared/utils.py`
2. O crear nuevo m√≥dulo en `shared/`
3. Exportar en `shared/__init__.py`

---

## üìù Notas

- Todos los evaluadores comparten las mismas clases (`EvaluationResult`, `EvaluatorConfig`, etc.)
- Las utilidades en `shared/` est√°n disponibles para todos
- Cada evaluador puede tener su propia configuraci√≥n adicional
- La interfaz `IEvaluator` es opcional (IterativeEvaluator no la implementa)
