# Infinite Web Arena (IWA)

## Synthetic Evaluation Benchmark for Web Agents

Welcome to **Infinite Web Arena (IWA)**, a revolutionary **autonomous web agent evaluation framework** that transcends traditional benchmarking limitations. Unlike existing benchmarks that rely on human-curated datasets and manual validation, IWA creates an **infinitely scalable evaluation environment** through **generative AI** and **synthetic data**. This automation enables continuous testing against novel web scenarios without human bottlenecks, ensuring comprehensive evaluation of web agents' capabilities.

---

## Core Features

### ğŸ”„ Dynamic Web Generation

- Meta-programming and LLMs create infinite variants of websites
- Continuously introduces new challenges that prevent memorization
- Ensures agents face realistic, evolving scenarios

### ğŸ¤– Automated Task & Test Generation

- LLMs autonomously produce tasks and corresponding tests
- No dependency on human task designers
- Generates validation criteria before execution

### ğŸŒ Browser Execution & Analysis

- Launches real browser instances for authentic web interaction
- Records and analyzes every agent action and DOM state
- Captures complete interaction flow for evaluation
- Enables deep inspection of agent decision-making

### ğŸ“Š Smart Evaluation System

- Combines predefined tests with LLM-based analysis
- Evaluates success through both quantitative and qualitative metrics
- Leverages the key insight that verifying a task is simpler than performing it
- Provides granular scoring across multiple success criteria

<br>

## ğŸ” The Key to Validation: Tests

The strength of IWA lies in its holistic testing methodology. By managing both frontend and backend environments, we can evaluate web agent behavior across multiple layers, ensuring a comprehensive assessment of their capabilities. Directly integrating GenAI into web agent validation introduces a circular dependency, as it requires the validation logic to surpass the intelligence of the agents being tested. The solution is to distill the problem to its core and anchor it to a logical combination of predefined "conditions" or "events." Ultimately, success is determined by a logical function of these conditions, which, when true, unambiguously defines success in the task.

### ğŸ–¥ï¸ Frontend Tests

- **DOM Analysis**: Inspect HTML structure changes and state transitions
- **Network Activity**: Monitor API calls and data exchanges
- **Visual Verification**: Compare screenshots and UI states
- **Browser Events**: Track JavaScript execution and user interactions

### âš™ï¸ Backend Tests

* **Event Tracking**: Capture backend event emissions
* **State Validation**: Verify database and system changes
* **Process Flow**: Confirm complete business logic execution
* **Custom Events**: Leverage controlled environment for deep inspection

---

### ğŸŒ Real-World Applications

While validation occurs in controlled environments, agents develop skills directly applicable to production websites:

- Navigate complex DOM structures
- Handle dynamic content loading
- Process real-world UI patterns
- Adapt to varying website architectures

---

## ğŸ’¡ Example Use Case

Consider a typical e-commerce task:

### 1ï¸âƒ£ Task Generation

```
Task: "Buy a red dress for less than $10"
Tests: Verify Purchase() event with parameters
      (item: "red dress", price < $10)
```

### 2ï¸âƒ£ Agent Execution

- Navigate site
- Search for product
- Apply filters
- Complete purchase

### 3ï¸âƒ£ Validation

* Verify correct item selection
* Check price constraints
* Confirm purchase completion

---

## ğŸš€ Getting Started

1. Clone the repository
2. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```
3. Run setup script:

   ```bash
   bash setup.sh
   ```

ğŸ“– For a detailed guide, check out the [Setup Guide](docs/guides/setup.md).

---

## ğŸ§ª Running the Benchmark

The benchmark evaluates different agents against generated tasks, tests, and websites. Configure and execute with ease:

### ğŸ› ï¸ Configuration Variables (in `benchmark.py`)

| Variable                     | Description                       | Example                  |
| ---------------------------- | --------------------------------- | ------------------------ |
| `PROJECTS_TO_RUN`            | List of demo projects to evaluate | `[demo_web_projects[0]]` |
| `PROMPT_PER_USE_CASE_CONST`  | Prompts per use case              | `1`                      |
| `PLOT_BENCHMARK_RESULTS`     | Plot result graphs                | `False`                  |
| `SAVE_EVALUATION_RESULTS`    | Save result JSONs                 | `False`                  |
| `USE_CACHED_TASKS_CONST`     | Use pre-generated tasks           | `False`                  |
| `USE_CACHED_SOLUTIONS_CONST` | Use precomputed agent solutions   | `False`                  |
| `EVALUATE_REAL_TASKS_CONST`  | Evaluate on real web URLs         | `False`                  |
| `RETURN_EVALUATION_GIF`      | Record UI interactions as GIF     | `True`                   |
| `LOG_FILE`                   | Benchmark log file path           | `"benchmark.log"`        |

### âš™ï¸ Example Agent Configuration

Edit the `AGENTS` list to define which agents to test:

```python
AGENTS = [
    ApifiedWebAgent(id="2", name="OpenAICUA", host="127.0.0.1", port=5005, timeout=300),
    # ApifiedWebAgent(id="3", name="AnthropicCUA", port=5010, ...)
]
```

Ensure all services are up before starting.

### â–¶ï¸ Run the Benchmark

```bash
python benchmark.py
```

### ğŸ“ Outputs

* **GIFs**: Saved in `recordings/<agent_name>/<task_id>.gif`
* **Logs**: Stored in `benchmark.log`
* **Metrics** (optional): JSONs and plots in `config.output_dir`

### ğŸ§¹ Reset Cache (Optional)

```bash
rm -rf ~/.autoppia_cache/tasks/
rm -rf ~/.autoppia_cache/solutions/
```

---

## ğŸ“œ License

Â© 2024 Autoppia. All rights reserved.

---

_Built with â¤ï¸ by the Autoppia Team_
