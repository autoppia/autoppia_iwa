# Infinite Web Arena (IWA)

## ğŸŒ What is Infinite Web Arena?

**Infinite Web Arena (IWA)** is a groundbreaking **autonomous web agent evaluation framework** that solves the fundamental scalability problem in web automation testing.

### The Problem with Traditional Benchmarks

Traditional web agent benchmarks face critical limitations:
- ğŸš« **Limited datasets**: Rely on manually curated tasks that agents can memorize
- ğŸš« **Human bottlenecks**: Require manual task creation and validation
- ğŸš« **Static environments**: Fixed websites that don't evolve
- ğŸš« **Expensive scaling**: Each new test scenario needs human intervention

### The IWA Solution

IWA creates an **infinitely scalable evaluation environment** where everything is automated:

- ğŸŒ **Infinite Websites**: Meta-programming and LLMs generate unlimited website variations
- ğŸ¯ **Infinite Tasks**: AI autonomously creates diverse, realistic web interaction tasks
- âœ… **Automated Validation**: Smart testing verifies success without human involvement
- ğŸ”„ **Continuous Evolution**: New challenges prevent memorization and ensure robust learning

**The result?** A self-sustaining evaluation ecosystem that can test web agents against an endless stream of novel scenarios, providing truly comprehensive capability assessment.

---

## ğŸ¯ Core Features

### ğŸ”„ Dynamic Web Generation

- **Meta-programming** and **LLMs** create infinite variants of websites
- Continuously introduces new challenges that prevent **memorization**
- Ensures agents face **realistic**, **evolving** scenarios

### ğŸ¤– Automated Task & Test Generation

- **LLMs** autonomously produce tasks and corresponding tests
- **No dependency** on human task designers
- Generates **validation criteria** before execution

### ğŸŒ Browser Execution & Analysis

- Launches **real browser instances** for authentic web interaction
- Records and analyzes every **agent action** and **DOM state**
- Captures complete **interaction flow** for evaluation
- Enables deep inspection of **agent decision-making**

### ğŸ“Š Smart Evaluation System

- Combines **predefined tests** with **LLM-based analysis**
- Evaluates success through both **quantitative** and **qualitative** metrics
- Leverages the key insight that **verifying a task is simpler than performing it**

---

## ğŸ§ª IWA Benchmark

The **IWA Benchmark** is your testing ground for developing and evaluating web agents. It provides a complete evaluation pipeline that simulates real-world validator behavior in a controlled environment.

### What the Benchmark Does

The benchmark orchestrates a complete evaluation workflow:

1. **ğŸ“‹ Task Generation**: Uses LLMs to generate diverse web interaction tasks for demo applications (e-commerce sites, CRMs, email clients, booking systems, etc.)

2. **ğŸ¤– Agent Execution**: Sends tasks to your agent(s) via HTTP API and collects their action sequences

3. **âœ… Validation & Scoring**: Executes agent actions in real browsers, monitors frontend/backend events, and evaluates success using predefined test criteria

4. **ğŸ“Š Performance Analysis**: Generates comprehensive metrics including success rates, execution times, and comparative reports between multiple agents

### Why Use the Benchmark?

- ğŸ¯ **Test before you deploy**: Validate your agent's capabilities without production risks
- ğŸ“Š **Compare implementations**: Run multiple agents side-by-side to identify the best approach
- ğŸ› **Debug efficiently**: Get detailed logs, GIF recordings, and error traces
- ğŸ’¡ **Understand scoring**: Learn how validators evaluate tasks to optimize your rewards
- ğŸ”„ **Iterate quickly**: Use cached tasks and solutions for faster development cycles

###  Test your agent


```bash
# Quick Test Run
cd autoppia_iwa_module
python -m autoppia_iwa.entrypoints.benchmark.run
```

**â†’ [Benchmark Guide](autoppia_iwa/entrypoints/benchmark/README.md)**

---


## ğŸ” How Validation Works: The Key is Testing

IWA's strength lies in its **holistic testing methodology**. By controlling both frontend and backend environments, we evaluate web agent behavior across multiple layers, ensuring comprehensive capability assessment.

### The Validation Challenge

**The problem**: Directly using GenAI for validation creates a circular dependencyâ€”the validator would need to be smarter than the agents being tested.

**The solution**: Distill validation to its essence through **predefined conditions and events**. Instead of asking an AI "did this work?", we define precise, logical criteria that unambiguously determine success.

Success is a **logical function of conditions**: when specific events fire with correct parameters, the task is objectively complete. This approach is both more reliable and more scalable than subjective evaluation.

### ğŸ–¥ï¸ Frontend Tests

- **DOM Analysis**: Inspect HTML structure changes and state transitions
- **Network Activity**: Monitor API calls and data exchanges
- **Visual Verification**: Compare screenshots and UI states
- **Browser Events**: Track JavaScript execution and user interactions

### âš™ï¸ Backend Tests

* **Event Tracking**: Capture backend event emissions
* **State Validation**: Verify database and system changes
* **Process Flow**: Confirm complete business logic execution
* **Custom Events**: Leverage controlled environment for deep inspection

---

### ğŸŒ Real-World Applications

While validation occurs in controlled environments, agents develop skills directly applicable to production websites:

- Navigate complex DOM structures
- Handle dynamic content loading
- Process real-world UI patterns
- Adapt to varying website architectures

---

## ğŸ’¡ Example Use Case

Consider a typical e-commerce task:

### 1ï¸âƒ£ Task Generation

```
Task: "Buy a red dress for less than $10"
Tests: Verify Purchase() event with parameters
      (item: "red dress", price < $10)
```

### 2ï¸âƒ£ Agent Execution

- Navigate site
- Search for product
- Apply filters
- Complete purchase

### 3ï¸âƒ£ Validation

* Verify correct item selection
* Check price constraints
* Confirm purchase completion

---

## ğŸ® Advanced Benchmark Features

The benchmark provides powerful capabilities for comprehensive agent development and testing:

### ğŸ“Š Output & Metrics

- âœ… **Success/failure rates**: Detailed scoring for each task and agent
- â±ï¸ **Execution time analysis**: Performance profiling and bottleneck identification
- ğŸ¬ **GIF recordings**: Visual playback of agent interactions
- ğŸ“ˆ **Comparison charts**: Side-by-side agent performance visualization
- ğŸ“ **Debug logs**: Complete traces of actions, events, and errors
- ğŸ’¾ **Smart caching**: Reuse tasks and solutions for faster iteration
- ğŸ“š **Sandbox analytics**: Ingest miner datasets and flag unresolved/trivial tasks with `python -m modules.web_verification.entrypoints.analyze_sandbox â€¦` (see `docs/guides/sandbox_analysis.md`)

### âš™ï¸ Customization Options

- **Multi-agent testing**: Compare different implementations simultaneously
- **Project selection**: Choose specific demo websites to evaluate
- **Parallel execution**: Configure concurrent agent calls for stress testing
- **Statistical runs**: Execute multiple iterations for robust metrics
- **Real-world evaluation**: Test against actual production websites

**â†’ See the comprehensive [Benchmark Guide](autoppia_iwa/entrypoints/benchmark/README.md)** for detailed configuration and usage instructions.
---
## ğŸ§­ Browser-use Agent

Run real browser actions during benchmarks using the lightweight `browser-use` integration.

### âš™ï¸ Setup
- Use the root setup script (installs Python deps and Playwright browsers):
  ```bash
  bash setup.sh
  ```
- Or install manually:
  ```bash
  pip install -r requirements.txt
  playwright install --with-deps
  ```

### ğŸš€ Quickstart
- Agent source: `autoppia_iwa/src/web_agents/browser-use/agent.py`
- Minimal usage inside IWA flows:
  ```python
  from autoppia_iwa.src.web_agents.browser_use.agent import BrowserUseWebAgent, BrowserUseConfig

  agent = BrowserUseWebAgent(BrowserUseConfig(headless=True))
  solution = await agent.solve_task(task)  # TaskSolution with recording
```

## ğŸŒ Server-side Mutation Proxy

Make every demo web dynamic directly at the HTTP layer: launch the FastAPI reverse proxy packaged under `modules/dynamic_proxy`.

```bash
python scripts/run_demo_web_proxy.py --config modules/dynamic_proxy/config.example.json
# or run via Docker
docker build -f modules/dynamic_proxy/Dockerfile -t autoppia-dynamic-proxy .
docker run --network=host \
  -e DYNAMIC_PROXY_CONFIG=/config/proxy_config.json \
  -v $(pwd)/modules/dynamic_proxy/config.example.json:/config/proxy_config.json \
  autoppia-dynamic-proxy
```

- Each config entry maps a public `listen_port` to the original demo web origin (`origin`).
- Responses flow through the same D1/D3/D4 engine used by the Dynamic executor, so seeds/palettes stay deterministic.
- Overlays are injected with a lightweight bootstrap script, forcing agents or humans to dismiss them just like in the evaluator.
- Audit artifacts land in `data/dynamic_proxy_audit/`, keeping before/after HTML and plan metadata for spot checks.

### ğŸ”§ Evaluation Notes
- Evaluators can replay/analyze actions; if needed, adapt `AgentHistoryList` into IWA `BaseAction` items before returning the `TaskSolution` (see the Browser-use readme for a sketch).

### ğŸ“– Learn More
- Full guide: [Browser-use readme](autoppia_iwa/src/web_agents/browser-use/readme.md)


---
## ğŸ†˜ Support & Contact

**Need help?** Contact our team on Discord:

- **@Daryxx**
- **@Riiveer**


Â© 2024 Autoppia. All rights reserved.

---
